{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play PPO with cart pole for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch as tc\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "sns.set_theme()\n",
    "DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, obs_space_size, action_space_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(obs_space_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_space_size)\n",
    "        )\n",
    "        \n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "    \n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "    \n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "    \n",
    "    def forward(self,obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer():\n",
    "    def __init__(self,\n",
    "                 actor_critic:ActorCriticNetwork,\n",
    "                 ppo_clip_value=0.2,\n",
    "                 target_kl_div=0.01,\n",
    "                 max_policy_training_iterations=80,\n",
    "                 value_training_iterations=80,\n",
    "                 policy_lr=3e-4,\n",
    "                 value_lr=1e-2):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_value = ppo_clip_value\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_training_iterations = max_policy_training_iterations\n",
    "        self.value_training_iterations = value_training_iterations\n",
    "        \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "        value_params = list(self.ac.shared_layers.parameters()) + list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, value_lr)\n",
    "        \n",
    "    def train_policy(self, obs, actions, old_log_probs, general_advantage_estimation):\n",
    "        \n",
    "        for _ in range(self.max_policy_training_iterations):\n",
    "            self.policy_optim.zero_grad()\n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits=new_logits)\n",
    "            new_logits_probs = new_logits.log_prob(actions)\n",
    "            policy_ratio = tc.exp(new_logits_probs - old_log_probs)\n",
    "            \n",
    "            clipped_ratio = policy_ratio.clamp(\n",
    "                1 - self.ppo_clip_value, 1 + self.ppo_clip_value\n",
    "            )\n",
    "            policy_loss = tc.min(policy_ratio, clipped_ratio)* general_advantage_estimation\n",
    "            \n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_divergence = (old_log_probs - new_logits).mean()\n",
    "            if kl_divergence >= self.target_kl_div:\n",
    "                break\n",
    "    \n",
    "    def train_value(self, obs, returns):\n",
    "        for _ in range(self.value_training_iterations):\n",
    "            self.value_optim.zero_grad()\n",
    "            values = self.ac.value(obs)\n",
    "            \n",
    "            value_loss = (returns - values)**2\n",
    "            value_loss = value_loss.mean()\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5, 3, 2, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 5, 6]\n",
    "data[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(rewards, gamma=0.99):\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    \n",
    "    for i in reversed(range(len(rewards) - 1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma*new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_general_advantage_estimates(rewards, values, gamma=0.99, decay=0.97):\n",
    "    '''\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/pdf/1506.02438.pdf\n",
    "    '''\n",
    "    next_values = np.concatenate([values[1:],[0]])\n",
    "    deltas = (reward + gamma*next_value - value for reward, value, next_value in zip(rewards, values, next_values))\n",
    "    general_advantage_estimates = [deltas[-1]]\n",
    "    \n",
    "    \n",
    "    for i in reversed(range(len(deltas) - 1)):\n",
    "        general_advantage_estimates.append(deltas[i] + decay*gamma*general_advantage_estimates[-1])\n",
    "    \n",
    "    return np.array(general_advantage_estimates[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000):\n",
    "    \n",
    "    #training data\n",
    "    \n",
    "    train_data = [[],[],[],[],[]] # save observation(0), action(1), reward(2), values(3) and action_log_probs\n",
    "    obs = env.reset()\n",
    "    \n",
    "    ep_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        logits, val = model(tc.tensor([obs[0]], dtype=tc.float32, device=DEVICE))\n",
    "        \n",
    "        action_distribution = Categorical(logits=logits)\n",
    "        action = action_distribution.sample()\n",
    "        action_log_probability = action_distribution.log_prob(action).item()\n",
    "        next_obs, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        #save the training data\n",
    "        # here we pack all training data into one item and append it to the lists\n",
    "        for i, item in enumerate((obs, action,reward, val, action_log_probability)):\n",
    "            train_data[i].append(item)\n",
    "            \n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "    train_data[3] = calculate_general_advantage_estimates(train_data[2], train_data[3])\n",
    "    return train_data, ep_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chunde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 2 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m ActorCriticNetwork(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m----> 4\u001b[0m train_data, reward \u001b[38;5;241m=\u001b[39m \u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m, in \u001b[0;36mrollout\u001b[1;34m(model, env, max_steps)\u001b[0m\n\u001b[0;32m      8\u001b[0m ep_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m---> 11\u001b[0m     logits, val \u001b[38;5;241m=\u001b[39m model(\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m,device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[0;32m     13\u001b[0m     action_distribution \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[0;32m     14\u001b[0m     action \u001b[38;5;241m=\u001b[39m action_distribution\u001b[38;5;241m.\u001b[39msample()\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 4 at dim 2 (got 0)"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "model = model.to(DEVICE)\n",
    "train_data, reward = rollout(model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setup PPO parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "\n",
    "ppo = PPOTrainer(model, policy_lr=3e-4, value_lr=1e-3, target_kl_div=0.02, max_policy_training_iterations=40, value_training_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rewards = []\n",
    "\n",
    "for episode_index in range(n_episodes):\n",
    "    train_data, reward = rollout(model=model, env=env)\n",
    "    ep_rewards.append(reward)\n",
    "    \n",
    "    # let us shuffle training data\n",
    "    permute_indics = np.random.permutation(len(train_data[0]))\n",
    "    permuted_obs = tc.tensor(train_data[0][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    permuted_values = tc.tensor(train_data[3][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    permuted_act = tc.tensor(train_data[1][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    permuted_act_log_probs = tc.tensor(train_data[4][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    \n",
    "    returns = discount_reward(train_data[2])[permute_indics]\n",
    "    returns = tc.tensor(returns, dtype=tc.float32, device=DEVICE)\n",
    "    \n",
    "    ppo.train_policy(permuted_obs, permuted_act, permuted_act_log_probs, permuted_values)\n",
    "    ppo.train_value(permuted_obs, returns)\n",
    "    if (episode_index - 1) % print_freq == 0:\n",
    "        print('Episode{} | Average Reward {:.1f}'.format(episode_index + 1, np.mean(ep_rewards[-print_freq:])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
