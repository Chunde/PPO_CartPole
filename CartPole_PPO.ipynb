{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play PPO with cart pole for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch as tc\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "sns.set_theme()\n",
    "DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, obs_space_size, action_space_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(obs_space_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_space_size)\n",
    "        )\n",
    "        \n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "    \n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "    \n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "    \n",
    "    def forward(self,obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer():\n",
    "    def __init__(self,\n",
    "                 actor_critic:ActorCriticNetwork,\n",
    "                 ppo_clip_value=0.2,\n",
    "                 target_kl_div=0.01,\n",
    "                 max_policy_training_iterations=80,\n",
    "                 value_training_iterations=80,\n",
    "                 policy_lr=3e-4,\n",
    "                 value_lr=1e-2):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_value = ppo_clip_value\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_training_iterations = max_policy_training_iterations\n",
    "        self.value_training_iterations = value_training_iterations\n",
    "        \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "        value_params = list(self.ac.shared_layers.parameters()) + list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, value_lr)\n",
    "        \n",
    "    def train_policy(self, obs, actions, old_log_probs, general_advantage_estimation):\n",
    "        \n",
    "        for _ in range(self.max_policy_training_iterations):\n",
    "            self.policy_optim.zero_grad()\n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits=new_logits)\n",
    "            new_logits_probs = new_logits.log_prob(actions)\n",
    "            policy_ratio = tc.exp(new_logits_probs - old_log_probs)\n",
    "            \n",
    "            clipped_ratio = policy_ratio.clamp(\n",
    "                1 - self.ppo_clip_value, 1 + self.ppo_clip_value\n",
    "            )\n",
    "            policy_loss = tc.min(policy_ratio, clipped_ratio)* general_advantage_estimation\n",
    "            \n",
    "            policy_loss.mean().backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_divergence = (old_log_probs - new_logits_probs).mean()\n",
    "            if kl_divergence >= self.target_kl_div:\n",
    "                break\n",
    "    \n",
    "    def train_value(self, obs, returns):\n",
    "        for _ in range(self.value_training_iterations):\n",
    "            self.value_optim.zero_grad()\n",
    "            values = self.ac.value(obs)\n",
    "            \n",
    "            value_loss = (returns - values)**2\n",
    "            value_loss = value_loss.mean()\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5, 3, 2, 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1, 2, 3, 5, 6]\n",
    "data[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(rewards, gamma=0.99):\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    \n",
    "    for i in reversed(range(len(rewards) - 1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma*new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_general_advantage_estimates(rewards, values, gamma=0.99, decay=0.97):\n",
    "    '''\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/pdf/1506.02438.pdf\n",
    "    '''\n",
    "    next_values = np.concatenate([values[1:],[0]])\n",
    "    deltas = [reward + gamma*next_value - value for reward, value, next_value in zip(rewards, values, next_values)]\n",
    "    general_advantage_estimates = [deltas[-1]]\n",
    "    \n",
    "    \n",
    "    for i in reversed(range(len(deltas) - 1)):\n",
    "        general_advantage_estimates.append(deltas[i] + decay*gamma*general_advantage_estimates[-1])\n",
    "    \n",
    "    return np.array(general_advantage_estimates[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000):\n",
    "    \n",
    "    #training data\n",
    "    \n",
    "    train_data = [[],[],[],[],[]] # save observation(0), action(1), reward(2), values(3) and action_log_probs\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    ep_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        logits, val = model(tc.tensor(obs, dtype=tc.float32, device=DEVICE))\n",
    "        \n",
    "        action_distribution = Categorical(logits=logits)\n",
    "        action = action_distribution.sample()\n",
    "        action_log_probability = action_distribution.log_prob(action).item()\n",
    "        next_obs, reward, done, _, _ = env.step(action.item())\n",
    "        \n",
    "        #save the training data\n",
    "        # here we pack all training data into one item and append it to the lists\n",
    "        for i, item in enumerate((\n",
    "                obs,\n",
    "                action.detach().numpy(),\n",
    "                reward,\n",
    "                val.detach().numpy()[0],\n",
    "                action_log_probability,\n",
    "            )):\n",
    "            train_data[i].append(item)\n",
    "            \n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "    train_data[3] = calculate_general_advantage_estimates(train_data[2], train_data[3])\n",
    "    return train_data, ep_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chunde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\Chunde\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "model = model.to(DEVICE)\n",
    "train_data, reward = rollout(model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setup PPO parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "\n",
    "ppo = PPOTrainer(model, policy_lr=3e-4, value_lr=1e-3, target_kl_div=0.02, max_policy_training_iterations=40, value_training_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Tensor' and 'Categorical'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m returns \u001b[38;5;241m=\u001b[39m discount_reward(train_data[\u001b[38;5;241m2\u001b[39m])[permute_indics]\n\u001b[0;32m     15\u001b[0m returns \u001b[38;5;241m=\u001b[39m tc\u001b[38;5;241m.\u001b[39mtensor(returns, dtype\u001b[38;5;241m=\u001b[39mtc\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpermuted_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermuted_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermuted_act_log_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermuted_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m ppo\u001b[38;5;241m.\u001b[39mtrain_value(permuted_obs, returns)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (episode_index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m print_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[26], line 38\u001b[0m, in \u001b[0;36mPPOTrainer.train_policy\u001b[1;34m(self, obs, actions, old_log_probs, general_advantage_estimation)\u001b[0m\n\u001b[0;32m     35\u001b[0m policy_loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 38\u001b[0m kl_divergence \u001b[38;5;241m=\u001b[39m (\u001b[43mold_log_probs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnew_logits\u001b[49m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kl_divergence \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_kl_div:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Tensor' and 'Categorical'"
     ]
    }
   ],
   "source": [
    "ep_rewards = []\n",
    "\n",
    "for episode_index in range(n_episodes):\n",
    "    train_data, reward = rollout(model=model, env=env)\n",
    "    ep_rewards.append(reward)\n",
    "    \n",
    "    # let us shuffle training data\n",
    "    permute_indics = np.random.permutation(len(train_data[0]))\n",
    "    permuted_obs = tc.tensor(train_data[0][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    permuted_values = tc.tensor(train_data[3][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    permuted_act = tc.tensor(train_data[1][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    permuted_act_log_probs = tc.tensor(train_data[4][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    \n",
    "    returns = discount_reward(train_data[2])[permute_indics]\n",
    "    returns = tc.tensor(returns, dtype=tc.float32, device=DEVICE)\n",
    "    \n",
    "    ppo.train_policy(permuted_obs, permuted_act, permuted_act_log_probs, permuted_values)\n",
    "    ppo.train_value(permuted_obs, returns)\n",
    "    if (episode_index - 1) % print_freq == 0:\n",
    "        print('Episode{} | Average Reward {:.1f}'.format(episode_index + 1, np.mean(ep_rewards[-print_freq:])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
