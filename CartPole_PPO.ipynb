{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play PPO with cart pole for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch as tc\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "sns.set()\n",
    "DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, obs_space_size, action_space_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(obs_space_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policy_layers = nn.Sequential(\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_space_size)\n",
    "        )\n",
    "        \n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "    \n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "    \n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "    \n",
    "    def forward(self,obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        value = self.value_layers(z)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer():\n",
    "    def __init__(self,\n",
    "                 actor_critic:ActorCriticNetwork,\n",
    "                 ppo_clip_value=0.2,\n",
    "                 target_kl_div=0.01,\n",
    "                 max_policy_training_iterations=80,\n",
    "                 value_training_iterations=80,\n",
    "                 policy_lr=3e-4,\n",
    "                 value_lr=1e-2):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_value = ppo_clip_value\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_training_iterations = max_policy_training_iterations\n",
    "        self.value_training_iterations = value_training_iterations\n",
    "        \n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "        value_params = list(self.ac.shared_layers.parameters()) + list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, value_lr)\n",
    "        \n",
    "    def train_policy(self, obs, actions, old_log_probs, general_advantage_estimation):\n",
    "        \n",
    "        for _ in range(self.max_policy_training_iterations):\n",
    "            self.policy_optim.zero_grad()\n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits=new_logits)\n",
    "            new_logits_probs = new_logits.log_prob(actions)\n",
    "            policy_ratio = tc.exp(new_logits_probs - old_log_probs)\n",
    "            \n",
    "            clipped_ratio = policy_ratio.clamp(\n",
    "                1 - self.ppo_clip_value, 1 + self.ppo_clip_value\n",
    "            )\n",
    "            policy_loss = tc.min(policy_ratio, clipped_ratio)* general_advantage_estimation\n",
    "            \n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_divergence = (old_log_probs - new_logits).mean()\n",
    "            if kl_divergence >= self.target_kl_div:\n",
    "                break\n",
    "    \n",
    "    def train_value(self, obs, returns):\n",
    "        for _ in range(self.value_training_iterations):\n",
    "            self.value_optim.zero_grad()\n",
    "            values = self.ac.value(obs)\n",
    "            \n",
    "            value_loss = (returns - values)**2\n",
    "            value_loss = value_loss.mean()\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 5, 6]\n",
    "data[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(rewards, gamma=0.99):\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    \n",
    "    for i in reversed(range(len(rewards) - 1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma*new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_general_advantage_estimates(rewards, values, gamma=0.99, decay=0.97):\n",
    "    '''\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/pdf/1506.02438.pdf\n",
    "    '''\n",
    "    next_values = np.concatenate([values[1:],[0]])\n",
    "    deltas = (reward + gamma*next_value - value for reward, value, next_value in zip(rewards, values, next_values))\n",
    "    general_advantage_estimates = [deltas[-1]]\n",
    "    \n",
    "    \n",
    "    for i in reversed(range(len(deltas) - 1)):\n",
    "        general_advantage_estimates.append(deltas[i] + decay*gamma*general_advantage_estimates[-1])\n",
    "    \n",
    "    return np.array(general_advantage_estimates[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000):\n",
    "    \n",
    "    #training data\n",
    "    \n",
    "    train_data = [[],[],[],[],[]] # save observation(0), action(1), reward(2), values(3) and action_log_probs\n",
    "    obs = env.reset()\n",
    "    \n",
    "    ep_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        logits, val = model(tc.tensor([obs], dtype=tc.float32),device=DEVICE)\n",
    "        \n",
    "        action_distribution = Categorical(logits=logits)\n",
    "        action = action_distribution.sample()\n",
    "        action_log_probability = action_distribution.log_prob(action).item()\n",
    "        next_obs, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        #save the training data\n",
    "        # here we pack all training data into one item and append it to the lists\n",
    "        for i, item in enumerate((obs, action,reward, val, action_log_probability)):\n",
    "            train_data[i].append(item)\n",
    "            \n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "    train_data[3] = calculate_general_advantage_estimates(train_data[2], train_data[3])\n",
    "    return train_data, ep_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "model = model.to(DEVICE)\n",
    "train_data, reward = rollout(model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup PPO parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "\n",
    "ppo = PPOTrainer(model, policy_lr=3e-4, value_lr=1e-3, target_kl_div=0.02, max_policy_training_iterations=40, value_training_iterations=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rewards = []\n",
    "\n",
    "for episode_index in range(n_episodes):\n",
    "    train_data, reward = rollout(model=model, env=env)\n",
    "    ep_rewards.append(reward)\n",
    "    \n",
    "    # let us shuffle training data\n",
    "    permute_indics = np.random.permutation(len(train_data[0]))\n",
    "    permuted_obs = tc.tensor(train_data[0][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    permuted_values = tc.tensor(train_data[3][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    permuted_act = tc.tensor(train_data[1][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    permuted_act_log_probs = tc.tensor(train_data[4][permute_indics], dtype=tc.float32, device=DEVICE)\n",
    "    \n",
    "    returns = discount_reward(train_data[2])[permute_indics]\n",
    "    returns = tc.tensor(returns, dtype=tc.float32, device=DEVICE)\n",
    "    \n",
    "    ppo.train_policy(permuted_obs, permuted_act, permuted_act_log_probs, permuted_values)\n",
    "    ppo.train_value(permuted_obs, returns)\n",
    "    if (episode_index - 1) % print_freq == 0:\n",
    "        print('Episode{} | Average Reward {:.1f}'.format(episode_index + 1, np.mean(ep_rewards[-print_freq:])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
